{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c69dae88",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-02T15:22:40.586630Z",
     "iopub.status.busy": "2025-09-02T15:22:40.586337Z",
     "iopub.status.idle": "2025-09-02T15:22:40.605774Z",
     "shell.execute_reply": "2025-09-02T15:22:40.604876Z",
     "shell.execute_reply.started": "2025-09-02T15:22:40.586610Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from compute_score import compute_all_data, compute_score\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import ast\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7565dea9-453e-4797-b7c6-c5aca336daa7",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-09-02T15:17:53.969116Z",
     "iopub.status.idle": "2025-09-02T15:17:53.969353Z",
     "shell.execute_reply": "2025-09-02T15:17:53.969226Z",
     "shell.execute_reply.started": "2025-09-02T15:17:53.969217Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "with open (\"test/Gemini-Flask-2.0.json\", \"r\") as f:\n",
    "    df = json.load(f)\n",
    "\n",
    "predictions = [item['llm_answer'] for item in df]\n",
    "ground_truths = [item['ground_truth'] for item in df]\n",
    "\n",
    "predictions= predictions[:4]\n",
    "ground_truths= ground_truths[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3951e398",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Tàu hoả chạy quanh đường ray trong vườn',\n",
       " 'Đoàn tàu nhỏ đi qua khu vườn lớn',\n",
       " 'Mô hình tàu hoả là một phần của khu vườn',\n",
       " 'Tàu hoả được đặt trong khu vườn',\n",
       " 'Đoàn tàu đồ chơi di chuyển trong vườn']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ground_truths[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3ac66a5b-0e47-4f5d-a21b-3e294606ff50",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-09-02T15:17:53.972877Z",
     "iopub.status.idle": "2025-09-02T15:17:53.973142Z",
     "shell.execute_reply": "2025-09-02T15:17:53.973016Z",
     "shell.execute_reply.started": "2025-09-02T15:17:53.973001Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Một mô hình đường sắt chạy qua một khu vườn được thiết kế phong cảnh.'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "874f656b",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'split'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[49], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mcompute_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mground_truths\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpredictions\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Thesis/Evaluation/VQA-Evaluation/compute_score.py:281\u001b[0m, in \u001b[0;36mcompute_score\u001b[0;34m(ground_truths, generation)\u001b[0m\n\u001b[1;32m      0\u001b[0m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "File \u001b[0;32m~/Documents/Thesis/Evaluation/VQA-Evaluation/compute_score.py:164\u001b[0m, in \u001b[0;36mbleu_score\u001b[0;34m(self, labels, pred)\u001b[0m\n\u001b[1;32m      0\u001b[0m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "File \u001b[0;32m~/Documents/Thesis/Evaluation/VQA-Evaluation/bleu/bleu.py:287\u001b[0m, in \u001b[0;36mBleu.compute_score\u001b[0;34m(self, gts, res)\u001b[0m\n\u001b[1;32m    284\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m(\u001b[38;5;28mtype\u001b[39m(ref) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mlist\u001b[39m)\n\u001b[1;32m    285\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m(\u001b[38;5;28mlen\u001b[39m(ref) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 287\u001b[0m     bleu_scorer \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (hypo[\u001b[38;5;241m0\u001b[39m], ref)\n\u001b[1;32m    289\u001b[0m \u001b[38;5;66;03m# score, scores = bleu_scorer.compute_score(option='shortest')\u001b[39;00m\n\u001b[1;32m    290\u001b[0m score, scores \u001b[38;5;241m=\u001b[39m bleu_scorer\u001b[38;5;241m.\u001b[39mcompute_score(option\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclosest\u001b[39m\u001b[38;5;124m'\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/Thesis/Evaluation/VQA-Evaluation/bleu/bleu.py:163\u001b[0m, in \u001b[0;36mBleuScorer.__iadd__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m'''add an instance (e.g., from another sentence).'''\u001b[39;00m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(other) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m:\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;66;03m## avoid creating new BleuScorer instances\u001b[39;00m\n\u001b[0;32m--> 163\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcook_append\u001b[49m\u001b[43m(\u001b[49m\u001b[43mother\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompatible(other), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mincompatible BLEUs.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/Documents/Thesis/Evaluation/VQA-Evaluation/bleu/bleu.py:107\u001b[0m, in \u001b[0;36mBleuScorer.cook_append\u001b[0;34m(self, test, refs)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m'''called by constructor and __iadd__ to avoid creating new instances.'''\u001b[39;00m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m refs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 107\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcrefs\u001b[38;5;241m.\u001b[39mappend(\u001b[43mcook_refs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrefs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m test \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    109\u001b[0m         cooked_test \u001b[38;5;241m=\u001b[39m cook_test(test, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcrefs[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n",
      "File \u001b[0;32m~/Documents/Thesis/Evaluation/VQA-Evaluation/bleu/bleu.py:33\u001b[0m, in \u001b[0;36mcook_refs\u001b[0;34m(refs, eff, n)\u001b[0m\n\u001b[1;32m     31\u001b[0m maxcounts \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ref \u001b[38;5;129;01min\u001b[39;00m refs:\n\u001b[0;32m---> 33\u001b[0m     rl, counts \u001b[38;5;241m=\u001b[39m \u001b[43mprecook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mref\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m     reflen\u001b[38;5;241m.\u001b[39mappend(rl)\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m (ngram, count) \u001b[38;5;129;01min\u001b[39;00m counts\u001b[38;5;241m.\u001b[39mitems():\n",
      "File \u001b[0;32m~/Documents/Thesis/Evaluation/VQA-Evaluation/bleu/bleu.py:16\u001b[0m, in \u001b[0;36mprecook\u001b[0;34m(s, n, out)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mprecook\u001b[39m(s, n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m     13\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Takes a string as input and returns an object that can be given to\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;124;03m    either cook_refs or cook_test. This is optional: cook_refs and cook_test\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;124;03m    can take string arguments as well.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m     words \u001b[38;5;241m=\u001b[39m \u001b[43ms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m()\n\u001b[1;32m     17\u001b[0m     counts \u001b[38;5;241m=\u001b[39m defaultdict(\u001b[38;5;28mint\u001b[39m)\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, n \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'split'"
     ]
    }
   ],
   "source": [
    "compute_score(ground_truths[1], predictions[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b42deb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 13514106,
     "datasetId": 8140572,
     "sourceId": 12869232,
     "sourceType": "datasetVersion"
    },
    {
     "databundleVersionId": 13598509,
     "datasetId": 8189907,
     "sourceId": 12942295,
     "sourceType": "datasetVersion"
    },
    {
     "databundleVersionId": 13577094,
     "modelInstanceId": 422018,
     "sourceId": 553710,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "auto_vivqa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
